{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQGK_Fw9zKZS"
      },
      "source": [
        "\n",
        "## The Exercise\n",
        "\n",
        "ExampleCo, Inc is gathering several types of data for its fleet of very expensive machines. They collect four kinds of time series data for each machine in their fleet. When a machine is operating in normal mode the data behaves in a fairly predictable way, but with a moderate amount of noise. Before a machine fails it will ramp into faulty mode, during which the data appears visibly quite different. Finally, when a machine fails it enters a third, and distinctly different, failed mode where all signals are very close to zero. There are four common sensors associated with each machine. There is also static data associated with each machine.\n",
        "\n",
        "ExampleCo stores their data in SQL tables. While SQL is useful for storing and querying data, it is less useful as input for machine learning pipelines. At Tagup, our ML Pipelines expect data as an n-dimensional array, and a common first step is converting a customer's data from SQL to array form.\n",
        "\n",
        "You can download the data here: [exampleco_data](https://drive.google.com/file/d/1GejVDBoFFVNprqMeTGnXu8hrYLj4aS4q/view?usp=sharing)\n",
        "\n",
        "Your objectives are:\n",
        "\n",
        "- Map the equipment data from SQL into arrays (or a single array). To keep things simple, you can ignore the static data to start. Be sure to describe your design choices and the array schema you chose.\n",
        "- Apply filters to clean the data. There are some clear outliers in the data due to communication errors from the sensor equipment. A good place to start is to find a way to filter them out.\n",
        "- Explain why your design and approach is effective, in language a non-technical executive can understand. If helpful, use visualizations to demonstrate the efficacy of your approach.\n",
        "\n",
        "Bonus points:\n",
        "- Integrate the static data into your design.\n",
        "- Provide summary statistics for the ingressed data, including various statistical moments, and any other relevant descriptive statistics.\n",
        "- As part of a data processing pipeline, upload your arrays to AWS S3.\n",
        "    \n",
        "A few notes to help:\n",
        "1. Feel free to use any libraries you like. At Tagup, we use [xarray](http://xarray.pydata.org/en/stable/) for multidimensional arrays, but you can use whatever methods you prefer. Your final results should be shared via GitHub, including a README file providing documentation (ideally of both your code and your findings in the data). There is a jupyter notebook included to help you get started, but do not feel obligated to submit your solution in notebook form.\n",
        "    \n",
        "2. There are no constraints on the techniques you bring to bear, we are curious to see how you think and what sort of resources you have in your toolbox.\n",
        "    \n",
        "3. Don't hesitate to reach out to challenges@tagup.io with any questions!\n",
        "    \n",
        "\n",
        "## Tips\n",
        "- We value syntax, structure and variable naming, code documentation, and testability.\n",
        "- Try to design and implement your solution as you would do for real production code. Show us how you create clean and maintainable code that produces the target result. Build something that we'd be happy to contribute to.\n",
        "- Feel free to add more features: we're curious about what you can think of. We'd expect the same if you worked with us!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlwpPoYHzKZc"
      },
      "source": [
        "# Load Database\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "9o8s7NBQzKZe"
      },
      "outputs": [],
      "source": [
        "# Note: Need to save this in github repo \"tagup\"\n",
        "\n",
        "# import libraries\n",
        "\n",
        "## manipulate labelled N-D arrays\n",
        "import xarray as xr\n",
        "\n",
        "## manipulate unlabelled N-D arrays with more matplotlib plotting support\n",
        "import pandas as pd\n",
        "\n",
        "## import and query from SQL databases\n",
        "from sqlalchemy import create_engine, inspect, MetaData, Table\n",
        "\n",
        "## perform fast array operations\n",
        "import numpy as np\n",
        "\n",
        "## plot from pandas dataframes\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If running in google colab, the following enables files to be accesed from and saved directly to a google drive folder. Adjust accordingly to save output files elsewhere.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "PARENT_DIR = '/content/drive/MyDrive/TagUpChallenge/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zc2Kixk6lD-L",
        "outputId": "218f565f-6606-4f0f-bc42-b107ca1fdbd7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WsTdsfPuzKZi"
      },
      "outputs": [],
      "source": [
        "# Load db. Replace PARENT_DIR with location of exampleco_db.db\n",
        "engine = create_engine('sqlite:///' + PARENT_DIR + 'exampleco_db.db', echo=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_SVQtPnzKZj",
        "outputId": "3b90b65a-b992-467a-f117-0c4114407cfd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['feat_0', 'feat_1', 'feat_2', 'feat_3', 'static_data']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# View tables\n",
        "inspector = inspect(engine)\n",
        "inspector.get_table_names()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convert Data to Arrays"
      ],
      "metadata": {
        "id": "0VsppjNp3ffQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert machine info to pandas DataFrame and save as CSV file\n",
        "\n",
        "## extract all data from static_data table\n",
        "query = \"SELECT * FROM static_data;\"\n",
        "static_data = engine.execute(query).fetchall()\n",
        "\n",
        "## save table data as pandas DataFrame\n",
        "machine_info = pd.DataFrame(static_data, columns=['machine', 'time', 'model', 'room'])\n",
        "\n",
        "## save \"time\" varibale as datetime type \n",
        "machine_info['time'] = pd.to_datetime(machine_info['time'])\n",
        "\n",
        "## save \"machine\" variable as integer\n",
        "machine_info['machine'] = machine_info['machine'].map(lambda s: s.split('_')[1]).astype(int)\n",
        "\n",
        "## save \"model\" variable as letter A or B\n",
        "machine_info['model'] = machine_info['model'].map(lambda s: s.split(' ')[1])\n",
        "\n",
        "## save \"room\" variable as integer\n",
        "machine_info['room'] = machine_info['room'].map(lambda s: s.split(' ')[1]).astype(int)\n",
        "\n",
        "## save DataFrame as CSV file\n",
        "machine_info.to_csv(PARENT_DIR + 'machine_info.csv')"
      ],
      "metadata": {
        "id": "vKeDOp2hYBLB"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# array of feature names\n",
        "feats = ['feat_0', 'feat_1', 'feat_2', 'feat_3']"
      ],
      "metadata": {
        "id": "dV4bpO3Hj600"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create an xarray DataArray for each feature\n",
        "\n",
        "## list of DataArrays for each feature\n",
        "xarrays = []\n",
        "\n",
        "for feat in feats:\n",
        "  ## select all data from table corresponding to feature\n",
        "  query = f\"SELECT * FROM {feat};\"\n",
        "  feat_data = engine.execute(query).fetchall()\n",
        "\n",
        "  ## save table data as a pandas DataFrame\n",
        "  feat_data = pd.DataFrame(feat_data, columns = ['time', 'machine', feat])\n",
        "\n",
        "  ## save \"time\" variable as a datetime type\n",
        "  feat_data['time'] = pd.to_datetime(feat_data['time'])\n",
        "\n",
        "  ## Save \"machine\" variable as integer. This forces plots to order facets correctly. (xarray does not appear to allow factor types)\n",
        "  feat_data['machine'] = feat_data['machine'].map(lambda s: s.split('_')[1]).astype(int)\n",
        "\n",
        "  ## convert table data from Pandas dataframe to xarray DataArray\n",
        "  feat_data = feat_data.set_index(['time', 'machine']).to_xarray()\n",
        "\n",
        "  ## append DataArray to list\n",
        "  xarrays.append(feat_data)\n",
        "\n",
        "## Merge list of DataArrays into a single DataSet with each feature as a DataArray\n",
        "data = xr.merge(xarrays)"
      ],
      "metadata": {
        "id": "rjEngyku-NWv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning"
      ],
      "metadata": {
        "id": "ERwKnwEf3rjx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Figures 1a-1d. Raw data over time, facetted by machine. Separate figures for each feature.\n",
        "for feat, fig in zip(feats, ['a','b','c','d']):\n",
        "  plt.figure()\n",
        "  data[feat].plot(x='time',col='machine', col_wrap=5)\n",
        "  plt.savefig(PARENT_DIR + '/figures/1'+ fig)"
      ],
      "metadata": {
        "id": "FuFhkR0bhagn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# figures 2a-2d. Histogram of data values by feature. Aggregated across machines and across time. Vertical lines represent outlier threshold.\n",
        "for feat, fig in zip(feats, ['a','b','c','d']):\n",
        "  plt.figure()\n",
        "  data[feat].plot.hist()\n",
        "  plt.title('Histogram of ' + feat + ' across all machines')\n",
        "  plt.axvline(150, color='k', linestyle='dashed', linewidth=1)\n",
        "  plt.axvline(-150, color='k', linestyle='dashed', linewidth=1)\n",
        "  plt.savefig(PARENT_DIR + '/figures/2'+ fig)"
      ],
      "metadata": {
        "id": "SjR6uDyTd_pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data cleaning - removal of outliers\n",
        "\n",
        "## array of cleaned feature names\n",
        "cleaned_feats = ['cleaned_' + feat for feat in feats]\n",
        "\n",
        "## define cleaned feature variables by removing values exceeding 150\n",
        "for feat, cfeat in zip(feats, cleaned_feats):\n",
        "  data[cfeat] = data[feat].where(abs(data[feat]) < 150)\n",
        "\n",
        "## compute percentage of data removed for each feature\n",
        "pct_outliers = [(xr.where(data[feat] > 150, 1, 0).sum()/data[feat].count()).item() for feat in feats]\n",
        "\n",
        "## convert outlier rates to pandas DataFrame\n",
        "pct_outlier_table = pd.DataFrame({'feature' : feats, 'Percent Outliers' : pct_outliers})\n",
        "\n",
        "## save outlier rates DataFrame as csv file\n",
        "pct_outlier_table.to_csv(PARENT_DIR + 'pct_outlier_table.csv')\n",
        "\n",
        "# f=open(PARENT_DIR + \"tables.txt\",\"w\")\n",
        "# f.write(pct_outlier_table.style.format({\n",
        "#     'Percent Outliers': '{:,.2%}'.format\n",
        "# }).render())\n",
        "# f.close()"
      ],
      "metadata": {
        "id": "LOhWx7wj6JIP"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VM2K0cfB3v4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Time-Series Visualization and Summary Statistics"
      ],
      "metadata": {
        "id": "nrtIY72R3ymU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Figure 3a-3d. Cleaned data over time, facetted by machine. Separate figures for each feature.\n",
        "for cfeat, fig in zip(cleaned_feats, ['a','b','c','d']):\n",
        "  plt.figure()\n",
        "  data[cfeat].plot(x='time',col='machine', col_wrap=5)\n",
        "  plt.savefig(PARENT_DIR + '/figures/3'+ fig)"
      ],
      "metadata": {
        "id": "ywgDhcC_RIvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute summary statistics \n",
        "\n",
        "## compute mean and standard deviation across time and machines for each feature\n",
        "means = data[cleaned_feats].mean(dim = ['time', 'machine']).to_pandas()\n",
        "stds = data[cleaned_feats].std(dim = ['time', 'machine']).to_pandas()\n",
        "\n",
        "## convert summary statistics to pandas DataFrame \n",
        "summary_stats = pd.DataFrame({'feature' : feats, 'Mean' : means, 'Standard Dev.' : stds})\n",
        "\n",
        "## save summary stats DataFrame as CSV file\n",
        "summary_stats.to_csv(PARENT_DIR + 'summary_stats.csv')"
      ],
      "metadata": {
        "id": "GqRMhf0v6Z_a"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Identification of Machine Failure"
      ],
      "metadata": {
        "id": "NEe2OAM94C7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# standard deviation over time\n",
        "\n",
        "## aggregate data by 2-week intervals and compute standard deviation for each machine and feature over time within these intervals\n",
        "data_by_week = data.resample(time=\"2W\").std(skipna = True)\n",
        "\n",
        "## convert to pandas dataframe to use seaborn library. Reset index to enable ploting by time and machine\n",
        "data_by_week_df = data_by_week[cleaned_feats].to_dataframe().reset_index()\n",
        "\n",
        "## convert from wide to long format to enable plotting all features on the same axes\n",
        "data_by_week_df_long = data_by_week_df.melt(id_vars=['time', 'machine'], value_vars = cleaned_feats, var_name='feature', value_name='standard deviation')\n",
        "\n",
        "## Figure 5. Standard deviation of feature values (across two-week intervals) over time. Facetted by machine. Color represents feature.\n",
        "sns.relplot(x = 'time', y = 'standard deviation', col = 'machine', data = data_by_week_df_long, col_wrap = 5, kind = 'line', hue = 'feature')\n",
        "plt.savefig(PARENT_DIR + '/figures/4')"
      ],
      "metadata": {
        "id": "vbZXw4Fmqqqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # plot standard deviation across each 2-week interval for each feature and machine\n",
        "# # figures 4a-4d\n",
        "# for cfeat, fig in zip(cleaned_feats, ['a','b','c','d']):\n",
        "#   plt.figure()\n",
        "#   data_by_week[cfeat].plot(x='time',col='machine', col_wrap=5)\n",
        "#   plt.savefig(PARENT_DIR + '/figures/4'+ fig)"
      ],
      "metadata": {
        "id": "bSdZXaDZhNh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Determine \"mode\" of each machine\n",
        "\n",
        "## Define the \"mode\" of the machine for each 2-week interval based on any feature's standard devation exceeding feature-specific threshold or being 0\n",
        "data_by_week['mode'] = xr.where((data_by_week['cleaned_feat_0'] > 30) | (data_by_week['cleaned_feat_1'] > 10) | \n",
        "                                (data_by_week['cleaned_feat_2'] > 20) | (data_by_week['cleaned_feat_3'] > 20), \n",
        "                                'faulty', \n",
        "                                xr.where((data_by_week['cleaned_feat_0'] == 0)| (data_by_week['cleaned_feat_1'] == 0) | \n",
        "                                         (data_by_week['cleaned_feat_2'] == 0) | (data_by_week['cleaned_feat_3'] == 0), \n",
        "                                         'failed', 'normal'))\n",
        "\n",
        "## Interplotate mode across two-week intervals to enable merging with original data\n",
        "mode = data_by_week.interp(time=data.time).mode\n",
        "\n",
        "## define mode as a DataArray in the original dataset\n",
        "data['mode'] = mode"
      ],
      "metadata": {
        "id": "Hyjr3b8XiRzM"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# figures 5a-5d. Cleaned data over time, facetted by machine. Separate figures for each feature. Colored by machine \"mode\".\n",
        "\n",
        "## convert to pandas dataframe to use seaborn library. Reset index to enable ploting by time and machine\n",
        "data_df = data.to_dataframe().reset_index()\n",
        "\n",
        "## colors for modes\n",
        "color_dict = dict({'normal':'green',\n",
        "                  'faulty':'orange',\n",
        "                  'failed': 'red'})\n",
        "\n",
        "for cfeat, fig in zip(cleaned_feats, ['a','b','c','d']):\n",
        "  plt.figure()\n",
        "  sns.relplot(x = 'time', y = cfeat, col = 'machine', data = data_df, col_wrap = 5, kind = 'line', hue = 'mode', palette=color_dict)\n",
        "  plt.savefig(PARENT_DIR + '/figures/5'+ fig)"
      ],
      "metadata": {
        "id": "dDV_HuwQZGTn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "39ee16c09488673965437987a2869652385c44c089bebde15996833fbab8938b"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "data_engineering_exercise.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}